# %%
import pandas as pd
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv


# Load environment variables
load_dotenv()

# Get database credentials from environment variables
db_host = os.getenv('DB_HOST')
db_port = os.getenv('DB_PORT')
db_name = os.getenv('DB_NAME')
db_user = os.getenv('DB_USER')
db_password = os.getenv('DB_PASSWORD')
        
# Create connection string
connection_string = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
# Connect to database
engine = create_engine(connection_string)

# %%
graded_df = pd.read_csv("../../audio-dataset/data/clean/graded_combined.csv")

# %%
filt_df = graded_df[['audio_file_name', 'Question', 'Human Transcription', 'profile_id']].drop_duplicates()

# %%
df_list = []
for i, row in filt_df.iterrows():
    profile_id = row['profile_id']
    question_text = row['Question'].replace("'", "''")  # Escape single quotes
    query = f"""
    SELECT war.*, saq.answer, saq."questionNumber", saq."difficultyLevel", 
    wum.city, wum."schoolName"
    FROM public.wa_question_responses war 
    LEFT JOIN public."speakActivityQuestions" saq ON saq.id = war."questionId"
    LEFT JOIN wa_users_metadata wum ON wum.profile_id = war.profile_id

    WHERE "activityType" IN ('assessmentWatchAndSpeak')
    AND war.profile_id = '{profile_id}'
    """
    df = pd.read_sql(query, engine)
    print("Processed: ", profile_id)

    df_list.append(df)

df_comb = pd.concat(df_list)

# %%
df_comb = pd.concat(df_list)

df_comb['question_clean'] = df_comb['answer'].str[0]
df_comb['ai_transcription'] = df_comb['submittedAnswerText'].str[0]


df_comb['submission_datetime_ai'] = df_comb['submissionDate'].astype(str).str.split(".").str[0]

# ...existing code...
df_comb['submission_datetime_ai'] = pd.to_datetime(df_comb['submission_datetime_ai'], errors='coerce')

# ---- simple group-by min/max -> assign pre/post/Other ----
# create a normalized submission_datetime column (coerce errors)
df_comb['submission_datetime'] = pd.to_datetime(df_comb['submissionDate'].astype(str).str.split(".").str[0], errors='coerce')

# compute min and max per profile_id + question_clean
minmax = df_comb.groupby(['profile_id', 'question_clean'])['submission_datetime'].agg(dt_min='min', dt_max='max').reset_index()

# merge min/max back onto df_comb
df_comb = df_comb.merge(minmax, on=['profile_id', 'question_clean'], how='left')

# label rows: earliest -> pre, latest -> post, others -> Other; single timestamp -> post; missing timestamp -> Other
def _label_pre_post(row):
    sd = row['submission_datetime']
    if pd.isna(sd):
        return 'Other'
    if pd.isna(row['dt_min']) or pd.isna(row['dt_max']):
        return 'Other'
    if row['dt_min'] == row['dt_max']:
        return 'post'
    if sd == row['dt_min']:
        return 'pre'
    if sd == row['dt_max']:
        return 'post'
    return 'Other'

df_comb['pre_or_post'] = df_comb.apply(_label_pre_post, axis=1)

# cleanup helper columns
df_comb.drop(columns=['submission_datetime', 'dt_min', 'dt_max'], inplace=True)

# %%
cols_to_keep = ['id', 'lessonId', 'questionId', 'profile_id', 'submissionDate', 'question_clean', 'ai_transcription',
                'submittedFeedbackJson', 'questionNumber', 'difficultyLevel', 
                'pre_or_post', 'city', 'schoolName']
df_final = df_comb[cols_to_keep]

# %%
df_final.to_csv("../../audio-dataset/data/clean/ai_responses_extracted.csv", index=False)
# %%
